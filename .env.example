# LLM Provider Configuration
LLM_PROVIDER=anthropic  # anthropic, openai, or local

# Anthropic (Claude)
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=claude-sonnet-4-20250514

# OpenAI (GPT-4)
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4-turbo

# Local LLM (Ollama)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=deepseek-coder:6.7b

# GitHub Access
GITHUB_TOKEN=ghp_...

# Server Settings
MAX_TOKENS_PER_CHUNK=100000
DEFAULT_ANALYSIS_DEPTH=deep
ENABLE_CACHING=true
CACHE_TTL_HOURS=24
MAX_DIAGRAM_NODES=50
MAX_CONCURRENT_REQUESTS=5
